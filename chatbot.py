import sys
import os
import torch
import tiktoken
import chainlit as cl

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from Utils.classes import GPTModel, generate

# ðŸ“Œ **Configuration du modÃ¨le**
config = {
    "vocab_size": 50257,
    "context_length": 256,
    "drop_rate": 0.1,
    "qkv_bias": False,
    "emb_dim": 768,
    "n_layers": 12,
    "n_heads": 12
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ðŸ“Œ **Chargement du modÃ¨le fine-tunÃ©**
model = GPTModel(config).to(device)
model.load_state_dict(torch.load("./Models/gpt_fr_finetuned.pth"))
model.eval()

# ðŸ“Œ **Initialisation du tokenizer**
tokenizer = tiktoken.get_encoding("gpt2")

# ðŸ“Œ **Interface utilisateur via Chainlit**
@cl.on_message
async def respond(message: cl.Message):
    user_input = message.content.strip()

    # Format du prompt
    prompt = f"Question : {user_input}\nRÃ©ponse :"

    # Tokenisation et gÃ©nÃ©ration
    encoded = torch.tensor([tokenizer.encode(prompt)]).to(device)
    generated_tokens = generate(model, encoded, max_new_tokens=100, context_size=config["context_length"], temperature=0.7, top_k=50)
    generated_text = tokenizer.decode(generated_tokens[0].tolist())

    # Nettoyage du texte gÃ©nÃ©rÃ©
    cleaned_text = generated_text.split("\nRÃ©ponse :")[1] if "\nRÃ©ponse :" in generated_text else generated_text
    cleaned_text = cleaned_text.split("<|endoftext|>")[0].strip()

    # RÃ©pondre Ã  l'utilisateur via Chainlit
    await cl.Message(content=cleaned_text).send()

# ðŸ“Œ **Lancer Chainlit**
if __name__ == "__main__":
    cl.run(port=8000, host="0.0.0.0", debug=True)
